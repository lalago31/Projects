{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "* Load the download and search log files with assigned schema. Examine the date and count distribution.\n",
    "* Transform the file_name/search time to proper date format for future feature engineering purpose\n",
    "* Join the download and search files with the valid_uid collected from the previos step play_log_preprocessing. Those uid will be the population we observe bahaviors within the one month time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Download log file and fix the dataframe schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Music Box down search log cleaning\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "down = sc.textFile(\"./data/all_down_log.log.fn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['167578852\\tar\\t7129012\\tFade(Dj6 Up Remix)\\t六月\\t0 \\t 1_down.l',\n",
       " '167928289\\tar\\t5060732\\t明天会更好 (六十位歌手大合唱)\\t群星\\t0 \\t 1_down.l',\n",
       " '167729304\\tar\\t472192\\t寂寞有多长\\t马条\\t0 \\t 1_down.l',\n",
       " '168022192\\tar\\t233916\\t年轻的战场(2006年《加油好男儿》主题曲)\\t张杰\\t0 \\t 1_down.l',\n",
       " '46532274\\tar\\t5398666\\t如果爱我只是因为你寂寞\\t高雅\\t0 \\t 1_down.l',\n",
       " '167666685\\tar\\t2961149\\t大气开场音乐\\t网络歌手\\t0 \\t 1_down.l',\n",
       " '167949944\\tar\\t3415466\\t像梦一样自由\\t张钰琪\\t0 \\t 1_down.l',\n",
       " '167664711\\tar\\t6700790\\t大王叫我来巡山-(电影《万万没想到\\t贾乃亮&甜馨\\t0 \\t 1_down.l',\n",
       " '167949944\\tar\\t4407436\\t给所有知道我名字的人\\t张钰琪\\t0 \\t 1_down.l',\n",
       " '167666685\\tar\\t5805490\\t最后的旅行——记《龙族》绘梨衣 (日文念白 Rainton桐)\\t网络歌手\\t0 \\t 1_down.l']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "down.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    fields = line.split(\"\\t\")\n",
    "    if len(fields) == 7:\n",
    "        try:\n",
    "            uid = float(fields[0])\n",
    "            device = str(fields[1])\n",
    "            song_id = float(fields[2])\n",
    "            song_name = str(fields[3])\n",
    "            singer = str(fields[4])\n",
    "            paid_flag = str(fields[5])\n",
    "            file_name = str(fields[6])\n",
    "            return Row(uid, device, song_id, song_name, singer, paid_flag, file_name)\n",
    "        except:\n",
    "            return Row(None)\n",
    "    else:\n",
    "        return Row(None)\n",
    "\n",
    "schema = StructType([StructField('uid', FloatType(), False),\n",
    "                       StructField('device', StringType(), True),\n",
    "                       StructField('song_id', FloatType(), False),\n",
    "                       StructField('song_name', StringType(), True),\n",
    "                       StructField('singer', StringType(), True),\n",
    "                       StructField('paid_flag', StringType(), True),\n",
    "                       StructField('file_name', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "down1=down.map(parseLine).filter(lambda x:len(x)==len(schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "down_df=spark.createDataFrame(down1,schema).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>device</th>\n",
       "      <th>song_id</th>\n",
       "      <th>song_name</th>\n",
       "      <th>singer</th>\n",
       "      <th>paid_flag</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167578848.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>7129012.0</td>\n",
       "      <td>Fade(Dj6 Up Remix)</td>\n",
       "      <td>六月</td>\n",
       "      <td>0</td>\n",
       "      <td>1_down.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167928288.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>5060732.0</td>\n",
       "      <td>明天会更好 (六十位歌手大合唱)</td>\n",
       "      <td>群星</td>\n",
       "      <td>0</td>\n",
       "      <td>1_down.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>167729312.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>472192.0</td>\n",
       "      <td>寂寞有多长</td>\n",
       "      <td>马条</td>\n",
       "      <td>0</td>\n",
       "      <td>1_down.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>168022192.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>233916.0</td>\n",
       "      <td>年轻的战场(2006年《加油好男儿》主题曲)</td>\n",
       "      <td>张杰</td>\n",
       "      <td>0</td>\n",
       "      <td>1_down.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46532272.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>5398666.0</td>\n",
       "      <td>如果爱我只是因为你寂寞</td>\n",
       "      <td>高雅</td>\n",
       "      <td>0</td>\n",
       "      <td>1_down.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>167666688.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>2961149.0</td>\n",
       "      <td>大气开场音乐</td>\n",
       "      <td>网络歌手</td>\n",
       "      <td>0</td>\n",
       "      <td>1_down.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>167949952.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>3415466.0</td>\n",
       "      <td>像梦一样自由</td>\n",
       "      <td>张钰琪</td>\n",
       "      <td>0</td>\n",
       "      <td>1_down.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>167664704.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>6700790.0</td>\n",
       "      <td>大王叫我来巡山-(电影《万万没想到</td>\n",
       "      <td>贾乃亮&amp;甜馨</td>\n",
       "      <td>0</td>\n",
       "      <td>1_down.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>167949952.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>4407436.0</td>\n",
       "      <td>给所有知道我名字的人</td>\n",
       "      <td>张钰琪</td>\n",
       "      <td>0</td>\n",
       "      <td>1_down.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>167666688.0</td>\n",
       "      <td>ar</td>\n",
       "      <td>5805490.0</td>\n",
       "      <td>最后的旅行——记《龙族》绘梨衣 (日文念白 Rainton桐)</td>\n",
       "      <td>网络歌手</td>\n",
       "      <td>0</td>\n",
       "      <td>1_down.log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           uid device    song_id                        song_name  singer  \\\n",
       "0  167578848.0     ar  7129012.0               Fade(Dj6 Up Remix)      六月   \n",
       "1  167928288.0     ar  5060732.0                 明天会更好 (六十位歌手大合唱)      群星   \n",
       "2  167729312.0     ar   472192.0                            寂寞有多长      马条   \n",
       "3  168022192.0     ar   233916.0           年轻的战场(2006年《加油好男儿》主题曲)      张杰   \n",
       "4   46532272.0     ar  5398666.0                      如果爱我只是因为你寂寞      高雅   \n",
       "5  167666688.0     ar  2961149.0                           大气开场音乐    网络歌手   \n",
       "6  167949952.0     ar  3415466.0                           像梦一样自由     张钰琪   \n",
       "7  167664704.0     ar  6700790.0                大王叫我来巡山-(电影《万万没想到  贾乃亮&甜馨   \n",
       "8  167949952.0     ar  4407436.0                       给所有知道我名字的人     张钰琪   \n",
       "9  167666688.0     ar  5805490.0  最后的旅行——记《龙族》绘梨衣 (日文念白 Rainton桐)    网络歌手   \n",
       "\n",
       "  paid_flag    file_name  \n",
       "0        0    1_down.log  \n",
       "1        0    1_down.log  \n",
       "2        0    1_down.log  \n",
       "3        0    1_down.log  \n",
       "4        0    1_down.log  \n",
       "5        0    1_down.log  \n",
       "6        0    1_down.log  \n",
       "7        0    1_down.log  \n",
       "8        0    1_down.log  \n",
       "9        0    1_down.log  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(down_df.take(10),columns=down_df.columns)\n",
    "#down_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the download table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### how is the file date distributed？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(file_name=' 1_down.log'),\n",
       " Row(file_name=' 20170330_1_down.log'),\n",
       " Row(file_name=' 20170330_2_down.log'),\n",
       " Row(file_name=' 20170330_3_down.log'),\n",
       " Row(file_name=' 20170331_1_down.log'),\n",
       " Row(file_name=' 20170331_2_down.log'),\n",
       " Row(file_name=' 20170331_3_down.log'),\n",
       " Row(file_name=' 20170401_1_down.log'),\n",
       " Row(file_name=' 20170401_2_down.log'),\n",
       " Row(file_name=' 20170401_3_down.log'),\n",
       " Row(file_name=' 20170402_1_down.log'),\n",
       " Row(file_name=' 20170402_2_down.log'),\n",
       " Row(file_name=' 20170402_3_down.log'),\n",
       " Row(file_name=' 20170403_1_down.log'),\n",
       " Row(file_name=' 20170403_2_down.log'),\n",
       " Row(file_name=' 20170403_3_down.log'),\n",
       " Row(file_name=' 20170404_1_down.log'),\n",
       " Row(file_name=' 20170404_2_down.log'),\n",
       " Row(file_name=' 20170404_3_down.log'),\n",
       " Row(file_name=' 20170405_1_down.log'),\n",
       " Row(file_name=' 20170405_2_down.log'),\n",
       " Row(file_name=' 20170405_3_down.log'),\n",
       " Row(file_name=' 20170406_1_down.log'),\n",
       " Row(file_name=' 20170406_2_down.log'),\n",
       " Row(file_name=' 20170406_3_down.log'),\n",
       " Row(file_name=' 20170407_1_down.log'),\n",
       " Row(file_name=' 20170407_2_down.log'),\n",
       " Row(file_name=' 20170407_3_down.log'),\n",
       " Row(file_name=' 20170408_1_down.log'),\n",
       " Row(file_name=' 20170408_2_down.log'),\n",
       " Row(file_name=' 20170408_3_down.log'),\n",
       " Row(file_name=' 20170409_1_down.log'),\n",
       " Row(file_name=' 20170409_2_down.log'),\n",
       " Row(file_name=' 20170409_3_down.log'),\n",
       " Row(file_name=' 20170410_1_down.log'),\n",
       " Row(file_name=' 20170410_2_down.log'),\n",
       " Row(file_name=' 20170410_3_down.log'),\n",
       " Row(file_name=' 20170411_1_down.log'),\n",
       " Row(file_name=' 20170411_2_down.log'),\n",
       " Row(file_name=' 20170412_1_down.log'),\n",
       " Row(file_name=' 20170412_2_down.log'),\n",
       " Row(file_name=' 20170412_3_down.log'),\n",
       " Row(file_name=' 20170413_1_down.log'),\n",
       " Row(file_name=' 20170413_2_down.log'),\n",
       " Row(file_name=' 20170413_3_down.log'),\n",
       " Row(file_name=' 20170414_1_down.log'),\n",
       " Row(file_name=' 20170414_2_down.log'),\n",
       " Row(file_name=' 20170414_3_down.log'),\n",
       " Row(file_name=' 20170415_1_down.log'),\n",
       " Row(file_name=' 20170415_2_down.log'),\n",
       " Row(file_name=' 20170415_3_down.log'),\n",
       " Row(file_name=' 20170416_1_down.log'),\n",
       " Row(file_name=' 20170416_2_down.log'),\n",
       " Row(file_name=' 20170416_3_down.log'),\n",
       " Row(file_name=' 20170417_1_down.log'),\n",
       " Row(file_name=' 20170417_2_down.log'),\n",
       " Row(file_name=' 20170417_3_down.log'),\n",
       " Row(file_name=' 20170418_1_down.log'),\n",
       " Row(file_name=' 20170418_2_down.log'),\n",
       " Row(file_name=' 20170418_3_down.log')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_dates_down=down_df.select('file_name').distinct().orderBy('file_name',ascending= True)\n",
    "file_dates_down.head(60)\n",
    "#download file dates start from 20170330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "down_df.createOrReplaceTempView('download')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|distinct_uid|\n",
      "+------------+\n",
      "|       90281|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select count(distinct uid) distinct_uid from download').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uid_count_down=spark.sql('select uid, count(uid) count from download group by uid order by 2 desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|         uid| count|\n",
      "+------------+------+\n",
      "|   1685126.0|292713|\n",
      "| 3.7025504E7|227318|\n",
      "|   1062806.0|150800|\n",
      "|   1791497.0|149346|\n",
      "|         0.0|142959|\n",
      "|    497685.0|113276|\n",
      "|    751824.0| 85865|\n",
      "|    736305.0| 74248|\n",
      "|   1749320.0| 43335|\n",
      "|   1679121.0| 26040|\n",
      "| 1.5594824E8| 24504|\n",
      "| 2.8638488E7| 23806|\n",
      "| 4.6532272E7| 21856|\n",
      "|    637650.0| 18754|\n",
      "|1.67703664E8|  9732|\n",
      "| 3.2166204E7|  9678|\n",
      "|1.68860544E8|  9486|\n",
      "|1.67713456E8|  9486|\n",
      "|1.60555088E8|  9373|\n",
      "| 6.4268008E7|  7974|\n",
      "+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uid_count_down.show() ## are the top 14 uid valid? values look too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|paid_flag|  count|\n",
      "+---------+-------+\n",
      "|       0 |8256882|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select paid_flag, count(1) count from download group by paid_flag').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* paid_flag only contains one value. can be discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exclude the first log because the file name cannot be converted to a date type \n",
    "down_df2=down_df.where(down_df['file_name']!=' 1_down.log')\n",
    "down_df2=down_df2.select('uid','song_id','file_name').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>song_id</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>168019808.0</td>\n",
       "      <td>442554.0</td>\n",
       "      <td>20170330_1_down.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168019808.0</td>\n",
       "      <td>6334611.0</td>\n",
       "      <td>20170330_1_down.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168019808.0</td>\n",
       "      <td>9867382.0</td>\n",
       "      <td>20170330_1_down.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>168019808.0</td>\n",
       "      <td>6660691.0</td>\n",
       "      <td>20170330_1_down.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>168019808.0</td>\n",
       "      <td>157606.0</td>\n",
       "      <td>20170330_1_down.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>168019808.0</td>\n",
       "      <td>3372481.0</td>\n",
       "      <td>20170330_1_down.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>168019808.0</td>\n",
       "      <td>3216525.0</td>\n",
       "      <td>20170330_1_down.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>168019808.0</td>\n",
       "      <td>6427523.0</td>\n",
       "      <td>20170330_1_down.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>168019808.0</td>\n",
       "      <td>6538686.0</td>\n",
       "      <td>20170330_1_down.log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>168019808.0</td>\n",
       "      <td>9327383.0</td>\n",
       "      <td>20170330_1_down.log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           uid    song_id             file_name\n",
       "0  168019808.0   442554.0   20170330_1_down.log\n",
       "1  168019808.0  6334611.0   20170330_1_down.log\n",
       "2  168019808.0  9867382.0   20170330_1_down.log\n",
       "3  168019808.0  6660691.0   20170330_1_down.log\n",
       "4  168019808.0   157606.0   20170330_1_down.log\n",
       "5  168019808.0  3372481.0   20170330_1_down.log\n",
       "6  168019808.0  3216525.0   20170330_1_down.log\n",
       "7  168019808.0  6427523.0   20170330_1_down.log\n",
       "8  168019808.0  6538686.0   20170330_1_down.log\n",
       "9  168019808.0  9327383.0   20170330_1_down.log"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(down_df2.take(10),columns=down_df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Load the valid uid table created from the previous play log files to filter the download log uid within the time window we want 2017-03-30 to 2017-05-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_uid=spark.read.csv('./data/valid_uid_1monthWindow/valid_uid_1monthWindow.csv',inferSchema= True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid_uid.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105351"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# play log valid uid count:\n",
    "valid_uid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "down_valid_uid=valid_uid.join(uid_count_down,on='uid',how='inner').drop(uid_count_down.uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|         uid|count|\n",
      "+------------+-----+\n",
      "|1.67703664E8| 9732|\n",
      "|1.68860544E8| 9486|\n",
      "|1.67713456E8| 9486|\n",
      "|1.60555088E8| 9373|\n",
      "|1.68333216E8| 7450|\n",
      "| 1.6892312E8| 7250|\n",
      "|1.68805296E8| 7143|\n",
      "| 1.6862512E8| 5990|\n",
      "|1.68690496E8| 5940|\n",
      "|1.67979376E8| 5850|\n",
      "|1.68849344E8| 5410|\n",
      "|1.68473152E8| 5249|\n",
      "|1.68301744E8| 5170|\n",
      "|1.67761888E8| 4858|\n",
      "|1.68363536E8| 4736|\n",
      "|1.68275776E8| 4714|\n",
      "|1.67739232E8| 4561|\n",
      "|1.68120768E8| 4141|\n",
      "|1.68442976E8| 4035|\n",
      "|1.68856192E8| 3994|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "down_valid_uid.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see all the uid with extremely large count are excluded after filtering out only valid uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89556"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "down_valid_uid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download uid/play log uid ratio= 0.8500726144032804\n"
     ]
    }
   ],
   "source": [
    "print('download uid/play log uid ratio= {}'.format(89556/105351))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get all the useful download log columns and convert the file name to proper date format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "down_df_clean=down_valid_uid.join(down_df2, on='uid',how='inner').drop(down_df2.uid)\\\n",
    "                            .withColumn('datestr',trim(down_df2.file_name).substr(1,9))\\\n",
    "                            .withColumn('unix_date',unix_timestamp('datestr', 'yyyyMMdd'))\\\n",
    "                            .withColumn('date',from_unixtime('unix_date').cast(DateType()))\\\n",
    "                            .drop('datestr').drop('unix_date').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>count</th>\n",
       "      <th>song_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4550267.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6196608.0</td>\n",
       "      <td>20170331_2_down.log</td>\n",
       "      <td>2017-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4550267.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6485492.0</td>\n",
       "      <td>20170331_2_down.log</td>\n",
       "      <td>2017-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4550267.0</td>\n",
       "      <td>3</td>\n",
       "      <td>859133.0</td>\n",
       "      <td>20170331_2_down.log</td>\n",
       "      <td>2017-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6216081.0</td>\n",
       "      <td>10</td>\n",
       "      <td>298180.0</td>\n",
       "      <td>20170404_2_down.log</td>\n",
       "      <td>2017-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6216081.0</td>\n",
       "      <td>10</td>\n",
       "      <td>157526.0</td>\n",
       "      <td>20170410_2_down.log</td>\n",
       "      <td>2017-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6216081.0</td>\n",
       "      <td>10</td>\n",
       "      <td>5040158.0</td>\n",
       "      <td>20170410_2_down.log</td>\n",
       "      <td>2017-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6216081.0</td>\n",
       "      <td>10</td>\n",
       "      <td>21769686.0</td>\n",
       "      <td>20170413_2_down.log</td>\n",
       "      <td>2017-04-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6216081.0</td>\n",
       "      <td>10</td>\n",
       "      <td>656643.0</td>\n",
       "      <td>20170413_2_down.log</td>\n",
       "      <td>2017-04-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6216081.0</td>\n",
       "      <td>10</td>\n",
       "      <td>3206899.0</td>\n",
       "      <td>20170420_2_down.log</td>\n",
       "      <td>2017-04-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6216081.0</td>\n",
       "      <td>10</td>\n",
       "      <td>541474.0</td>\n",
       "      <td>20170421_2_down.log</td>\n",
       "      <td>2017-04-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6216081.0</td>\n",
       "      <td>10</td>\n",
       "      <td>148209.0</td>\n",
       "      <td>20170426_2_down.log</td>\n",
       "      <td>2017-04-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6216081.0</td>\n",
       "      <td>10</td>\n",
       "      <td>21766068.0</td>\n",
       "      <td>20170427_2_down.log</td>\n",
       "      <td>2017-04-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6216081.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1521504.0</td>\n",
       "      <td>20170430_2_down.log</td>\n",
       "      <td>2017-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6869869.0</td>\n",
       "      <td>1</td>\n",
       "      <td>176322.0</td>\n",
       "      <td>20170413_1_down.log</td>\n",
       "      <td>2017-04-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>22553002.0</td>\n",
       "      <td>3</td>\n",
       "      <td>20929864.0</td>\n",
       "      <td>20170501_2_down.log</td>\n",
       "      <td>2017-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>22553002.0</td>\n",
       "      <td>3</td>\n",
       "      <td>11416998.0</td>\n",
       "      <td>20170501_2_down.log</td>\n",
       "      <td>2017-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>22553002.0</td>\n",
       "      <td>3</td>\n",
       "      <td>9883448.0</td>\n",
       "      <td>20170501_2_down.log</td>\n",
       "      <td>2017-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>57077508.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3432288.0</td>\n",
       "      <td>20170416_1_down.log</td>\n",
       "      <td>2017-04-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>57077508.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3385963.0</td>\n",
       "      <td>20170416_1_down.log</td>\n",
       "      <td>2017-04-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>57077508.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6525213.0</td>\n",
       "      <td>20170416_1_down.log</td>\n",
       "      <td>2017-04-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           uid  count     song_id             file_name        date\n",
       "0    4550267.0      3   6196608.0   20170331_2_down.log  2017-03-31\n",
       "1    4550267.0      3   6485492.0   20170331_2_down.log  2017-03-31\n",
       "2    4550267.0      3    859133.0   20170331_2_down.log  2017-03-31\n",
       "3    6216081.0     10    298180.0   20170404_2_down.log  2017-04-04\n",
       "4    6216081.0     10    157526.0   20170410_2_down.log  2017-04-10\n",
       "5    6216081.0     10   5040158.0   20170410_2_down.log  2017-04-10\n",
       "6    6216081.0     10  21769686.0   20170413_2_down.log  2017-04-13\n",
       "7    6216081.0     10    656643.0   20170413_2_down.log  2017-04-13\n",
       "8    6216081.0     10   3206899.0   20170420_2_down.log  2017-04-20\n",
       "9    6216081.0     10    541474.0   20170421_2_down.log  2017-04-21\n",
       "10   6216081.0     10    148209.0   20170426_2_down.log  2017-04-26\n",
       "11   6216081.0     10  21766068.0   20170427_2_down.log  2017-04-27\n",
       "12   6216081.0     10   1521504.0   20170430_2_down.log  2017-04-30\n",
       "13   6869869.0      1    176322.0   20170413_1_down.log  2017-04-13\n",
       "14  22553002.0      3  20929864.0   20170501_2_down.log  2017-05-01\n",
       "15  22553002.0      3  11416998.0   20170501_2_down.log  2017-05-01\n",
       "16  22553002.0      3   9883448.0   20170501_2_down.log  2017-05-01\n",
       "17  57077508.0      4   3432288.0   20170416_1_down.log  2017-04-16\n",
       "18  57077508.0      4   3385963.0   20170416_1_down.log  2017-04-16\n",
       "19  57077508.0      4   6525213.0   20170416_1_down.log  2017-04-16"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(down_df_clean.take(20),columns=down_df_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#file name is correctly converted to date format. now count and file_name can be dropped\n",
    "down_df_clean=down_df_clean.drop('count').drop('file_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|      date|  count|\n",
      "+----------+-------+\n",
      "|      null| 464933|\n",
      "|2017-03-30|1439545|\n",
      "|2017-03-31| 511630|\n",
      "|2017-04-01| 314203|\n",
      "|2017-04-02| 260032|\n",
      "|2017-04-03| 209962|\n",
      "|2017-04-04| 208517|\n",
      "|2017-04-05| 163680|\n",
      "|2017-04-06| 153144|\n",
      "|2017-04-07| 137793|\n",
      "|2017-04-08| 148584|\n",
      "|2017-04-09| 146677|\n",
      "|2017-04-10| 125722|\n",
      "|2017-04-11|  75087|\n",
      "|2017-04-12| 107735|\n",
      "|2017-04-13| 100620|\n",
      "|2017-04-14| 100013|\n",
      "|2017-04-15| 107840|\n",
      "|2017-04-16| 105240|\n",
      "|2017-04-17|  93764|\n",
      "+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "down_df_clean.groupBy('date').count().orderBy('date',ascending=True).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 03/30 seems to have abnormaly large download count in one day. I suspect there is problem in how the files are dated or maybe because musicbox users renew the subscription every month end, which results in large traffic on that day. I'll exclude null and keep 03/30 records for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "down_df_clean2=down_df_clean.where(down_df_clean['date']>='2017-03-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|      date|  count|\n",
      "+----------+-------+\n",
      "|2017-03-30|1439545|\n",
      "|2017-03-31| 511630|\n",
      "|2017-04-01| 314203|\n",
      "|2017-04-02| 260032|\n",
      "|2017-04-03| 209962|\n",
      "|2017-04-04| 208517|\n",
      "|2017-04-05| 163680|\n",
      "|2017-04-06| 153144|\n",
      "|2017-04-07| 137793|\n",
      "|2017-04-08| 148584|\n",
      "|2017-04-09| 146677|\n",
      "|2017-04-10| 125722|\n",
      "|2017-04-11|  75087|\n",
      "|2017-04-12| 107735|\n",
      "|2017-04-13| 100620|\n",
      "|2017-04-14| 100013|\n",
      "|2017-04-15| 107840|\n",
      "|2017-04-16| 105240|\n",
      "|2017-04-17|  93764|\n",
      "|2017-04-18|  91890|\n",
      "+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "down_df_clean2.groupBy('date').count().orderBy('date',ascending=True).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89556"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "down_df_clean2.select('uid').distinct().count() #excluding null dates does not drop the unique uid count. good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+\n",
      "|uid|song_id|date|\n",
      "+---+-------+----+\n",
      "+---+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "down_df_clean.where(down_df_clean['date'] == 'null').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+\n",
      "|        uid|    song_id|      date|\n",
      "+-----------+-----------+----------+\n",
      "|  4550267.0|  6196608.0|2017-03-31|\n",
      "|  4550267.0|  6485492.0|2017-03-31|\n",
      "|  4550267.0|   859133.0|2017-03-31|\n",
      "|  6216081.0|   298180.0|2017-04-04|\n",
      "|  6216081.0|   157526.0|2017-04-10|\n",
      "|  6216081.0|  5040158.0|2017-04-10|\n",
      "|  6216081.0|2.1769686E7|2017-04-13|\n",
      "|  6216081.0|   656643.0|2017-04-13|\n",
      "|  6216081.0|  3206899.0|2017-04-20|\n",
      "|  6216081.0|   541474.0|2017-04-21|\n",
      "|  6216081.0|   148209.0|2017-04-26|\n",
      "|  6216081.0|2.1766068E7|2017-04-27|\n",
      "|  6216081.0|  1521504.0|2017-04-30|\n",
      "|  6869869.0|   176322.0|2017-04-13|\n",
      "|2.2553002E7|2.0929864E7|2017-05-01|\n",
      "|2.2553002E7|1.1416998E7|2017-05-01|\n",
      "|2.2553002E7|  9883448.0|2017-05-01|\n",
      "|5.7077508E7|  3432288.0|2017-04-16|\n",
      "|5.7077508E7|  3385963.0|2017-04-16|\n",
      "|5.7077508E7|  6525213.0|2017-04-16|\n",
      "+-----------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "down_df_clean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the table to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "down_df_clean.repartition(1).write.csv('./data/down_df_clean',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search = sc.textFile(\"./data/all_search_log.log.fn\") # size = 1.02GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['154436633 \\tip \\t2017-03-01 00:00:24 \\t%e9%83%ad%e5%be%b7%e7%ba%b2 \\t 1_1_search.log',\n",
       " '154407262 \\tar \\t2017-03-01 00:00:53 \\t%E6%AF%AF%E5%AD%90%E8%88%9E \\t 1_1_search.log',\n",
       " '154407854 \\tip \\t2017-03-01 00:00:54 \\t%e7%96%a4%2d%20%28%e7%94%b5%e8%a7%86%e5%89%a7%e3%80%8a%e9%a3%9e%e5%88%80%e5%8f%88%e8%a7%81%e9%a3%9e%e5%88%80%e3%80%8b%e4%b8%bb%e9%a2%98%e6%9b%b2%29 \\t 1_1_search.log',\n",
       " '154407252 \\tar \\t2017-03-01 00:00:55 \\t%E6%88%91%E8%A6%81%E5%88%9B%E4%B8%9A++%E5%94%90%E8%8E%89%E7%BE%A4 \\t 1_1_search.log',\n",
       " '154407327 \\tar \\t2017-03-01 00:00:55 \\t%E4%B8%AB%E5%A4%B4++%E7%8E%8B%E7%AB%A5%E8%AF%AD \\t 1_1_search.log',\n",
       " '154407255 \\tip \\t2017-03-01 00:00:56 \\t%e5%8a%a8%e5%bf%83 \\t 1_1_search.log',\n",
       " '154407261 \\tip \\t2017-03-01 00:00:59 \\t%e8%83%8e%e6%95%99 \\t 1_1_search.log',\n",
       " '154407267 \\tar \\t2017-03-01 00:00:59 \\t%E5%81%B7%E6%83%85++mc%E5%8D%8A%E9%98%B3 \\t 1_1_search.log',\n",
       " '154407546 \\tip \\t2017-03-01 00:01:00 \\t%e8%96%9b%e4%b9%8b%e8%b0%a6 \\t 1_1_search.log',\n",
       " '154407254 \\tar \\t2017-03-01 00:01:02 \\t%E6%9D%A5%E7%94%B5%E9%93%83%E9%9F%B3 \\t 1_1_search.log']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the dataframe schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    fields = line.split(\"\\t\")\n",
    "    if len(fields) == 5:\n",
    "        try:\n",
    "            uid = float(fields[0])\n",
    "            device = str(fields[1])\n",
    "            search_time = str(fields[2])\n",
    "            url = str(fields[3])\n",
    "            file_name = str(fields[4])\n",
    "            return Row(uid, device, search_time, url, file_name)\n",
    "        except:\n",
    "            return Row(None)\n",
    "    else:\n",
    "        return Row(None)\n",
    "\n",
    "schema = StructType([StructField('uid', FloatType(), False),\n",
    "                     StructField('device', StringType(), True),\n",
    "                     StructField('search_time', StringType(), False),\n",
    "                     StructField('url', StringType(), True),\n",
    "                     StructField('file_name', StringType(), True)])\n",
    "#len(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search2=search.map(parseLine).filter(lambda x:len(x)==len(schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_df= spark.createDataFrame(search2, schema).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+--------------------+--------------------+---------------+\n",
      "|         uid|device|         search_time|                 url|      file_name|\n",
      "+------------+------+--------------------+--------------------+---------------+\n",
      "| 1.5443664E8|   ip |2017-03-01 00:00:24 |%e9%83%ad%e5%be%b...| 1_1_search.log|\n",
      "|1.54407264E8|   ar |2017-03-01 00:00:53 |%E6%AF%AF%E5%AD%9...| 1_1_search.log|\n",
      "|1.54407856E8|   ip |2017-03-01 00:00:54 |%e7%96%a4%2d%20%2...| 1_1_search.log|\n",
      "|1.54407248E8|   ar |2017-03-01 00:00:55 |%E6%88%91%E8%A6%8...| 1_1_search.log|\n",
      "|1.54407328E8|   ar |2017-03-01 00:00:55 |%E4%B8%AB%E5%A4%B...| 1_1_search.log|\n",
      "|1.54407248E8|   ip |2017-03-01 00:00:56 | %e5%8a%a8%e5%bf%83 | 1_1_search.log|\n",
      "|1.54407264E8|   ip |2017-03-01 00:00:59 | %e8%83%8e%e6%95%99 | 1_1_search.log|\n",
      "|1.54407264E8|   ar |2017-03-01 00:00:59 |%E5%81%B7%E6%83%8...| 1_1_search.log|\n",
      "|1.54407552E8|   ip |2017-03-01 00:01:00 |%e8%96%9b%e4%b9%8...| 1_1_search.log|\n",
      "|1.54407248E8|   ar |2017-03-01 00:01:02 |%E6%9D%A5%E7%94%B...| 1_1_search.log|\n",
      "|  1.544072E8|   ar |2017-03-01 00:01:06 |%E5%9B%A0%E4%B8%B...| 1_1_search.log|\n",
      "|1.54407248E8|   ar |2017-03-01 00:01:06 |%E6%83%85%E4%B8%8...| 1_1_search.log|\n",
      "|1.54407264E8|   ip |2017-03-01 00:01:07 |%e8%83%8e%e6%95%9...| 1_1_search.log|\n",
      "| 1.5440736E8|   ar |2017-03-01 00:01:08 |%E5%A4%A7%E8%AF%9...| 1_1_search.log|\n",
      "|1.54407376E8|   ar |2017-03-01 00:01:08 |%E5%88%AB%E6%80%9...| 1_1_search.log|\n",
      "|1.54407344E8|   ar |2017-03-01 00:01:09 |%E4%B8%89%E7%94%9...| 1_1_search.log|\n",
      "|1.54407296E8|   ar |2017-03-01 00:01:14 |%E4%B8%9C%E6%96%B...| 1_1_search.log|\n",
      "|1.54407408E8|   ar |2017-03-01 00:01:14 |%E4%B8%BA%E4%BA%8...| 1_1_search.log|\n",
      "| 1.5440736E8|   ar |2017-03-01 00:01:15 |%E7%9B%B8%E4%BA%B...| 1_1_search.log|\n",
      "|1.54407328E8|   ar |2017-03-01 00:01:17 |%E6%88%90%E9%83%B...| 1_1_search.log|\n",
      "+------------+------+--------------------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect search file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_df.createOrReplaceTempView('search_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* how many unique uid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|distinct_uid|\n",
      "+------------+\n",
      "|      128375|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct uid) distinct_uid from search_df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what's the distribution of uid count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|         uid| count|\n",
      "+------------+------+\n",
      "|         0.0|153472|\n",
      "|    497685.0|147226|\n",
      "|    736305.0|142533|\n",
      "| 3.7025504E7|100556|\n",
      "|   1791497.0| 94932|\n",
      "|   1062806.0| 60723|\n",
      "| 2.8638488E7| 33819|\n",
      "|   1679121.0| 24202|\n",
      "|    637650.0| 18126|\n",
      "| 6.4268008E7| 10526|\n",
      "| 3.2166204E7|  9516|\n",
      "|1.67724192E8|  7337|\n",
      "| 4.6532272E7|  5037|\n",
      "| 2.7954504E7|  4214|\n",
      "| 1.6517426E7|  4168|\n",
      "|1.68471616E8|  4042|\n",
      "|   1710083.0|  3309|\n",
      "| 1.5594824E8|  3058|\n",
      "|1.68461664E8|  2988|\n",
      "|   1685126.0|  2977|\n",
      "+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_uid_count=spark.sql(\"select uid, count(1) count from search_df group by uid order by 2 desc\")\n",
    "search_uid_count.show() \n",
    "## looks normal. the first uid=0 will be excluded after joining to valid_uid table from play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what's the distribution of file dates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(file_name=' 1_1_search.log'),\n",
       " Row(file_name=' 1_2_search.log'),\n",
       " Row(file_name=' 1_3_search.log'),\n",
       " Row(file_name=' 1_4_search.log'),\n",
       " Row(file_name=' 1_search'),\n",
       " Row(file_name=' 20170330_1_search.log'),\n",
       " Row(file_name=' 20170330_2_search.log'),\n",
       " Row(file_name=' 20170330_3_search.log'),\n",
       " Row(file_name=' 20170331_1_search.log'),\n",
       " Row(file_name=' 20170331_2_search.log'),\n",
       " Row(file_name=' 20170331_3_search.log'),\n",
       " Row(file_name=' 20170401_1_search.log'),\n",
       " Row(file_name=' 20170401_2_search.log'),\n",
       " Row(file_name=' 20170401_3_search.log'),\n",
       " Row(file_name=' 20170402_1_search.log'),\n",
       " Row(file_name=' 20170402_2_search.log'),\n",
       " Row(file_name=' 20170402_3_search.log'),\n",
       " Row(file_name=' 20170403_1_search.log'),\n",
       " Row(file_name=' 20170403_2_search.log'),\n",
       " Row(file_name=' 20170403_3_search.log'),\n",
       " Row(file_name=' 20170404_1_search.log'),\n",
       " Row(file_name=' 20170404_2_search.log'),\n",
       " Row(file_name=' 20170404_3_search.log'),\n",
       " Row(file_name=' 20170405_1_search.log'),\n",
       " Row(file_name=' 20170405_2_search.log'),\n",
       " Row(file_name=' 20170405_3_search.log'),\n",
       " Row(file_name=' 20170406_1_search.log'),\n",
       " Row(file_name=' 20170406_2_search.log'),\n",
       " Row(file_name=' 20170406_3_search.log'),\n",
       " Row(file_name=' 20170407_1_search.log'),\n",
       " Row(file_name=' 20170407_2_search.log'),\n",
       " Row(file_name=' 20170407_3_search.log'),\n",
       " Row(file_name=' 20170408_1_search.log'),\n",
       " Row(file_name=' 20170408_2_search.log'),\n",
       " Row(file_name=' 20170408_3_search.log'),\n",
       " Row(file_name=' 20170409_1_search.log'),\n",
       " Row(file_name=' 20170409_2_search.log'),\n",
       " Row(file_name=' 20170409_3_search.log'),\n",
       " Row(file_name=' 20170410_1_search.log'),\n",
       " Row(file_name=' 20170410_2_search.log'),\n",
       " Row(file_name=' 20170410_3_search.log'),\n",
       " Row(file_name=' 20170411_1_search.log'),\n",
       " Row(file_name=' 20170411_2_search.log'),\n",
       " Row(file_name=' 20170412_1_search.log'),\n",
       " Row(file_name=' 20170412_2_search.log'),\n",
       " Row(file_name=' 20170412_3_search.log'),\n",
       " Row(file_name=' 20170413_1_search.log'),\n",
       " Row(file_name=' 20170413_2_search.log'),\n",
       " Row(file_name=' 20170413_3_search.log'),\n",
       " Row(file_name=' 20170414_1_search.log'),\n",
       " Row(file_name=' 20170414_2_search.log'),\n",
       " Row(file_name=' 20170414_3_search.log'),\n",
       " Row(file_name=' 20170415_1_search.log'),\n",
       " Row(file_name=' 20170415_2_search.log'),\n",
       " Row(file_name=' 20170415_3_search.log'),\n",
       " Row(file_name=' 20170416_1_search.log'),\n",
       " Row(file_name=' 20170416_2_search.log'),\n",
       " Row(file_name=' 20170416_3_search.log'),\n",
       " Row(file_name=' 20170417_1_search.log'),\n",
       " Row(file_name=' 20170417_2_search.log')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_dates_src=search_df.select('file_name').distinct().orderBy('file_name',ascending= True)\n",
    "file_dates_src.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_count=spark.sql(\"select file_name, count(1) count \\\n",
    "                     from search_df \\\n",
    "                     where trim(file_name) not in (' 1_1_search.log',' 1_2_search.log',' 1_3_search.log',' 1_4_search.log',' 1_search')\\\n",
    "                     group by file_name order by 1 desc\")\n",
    "# file_count2=file_count.filter((file_count.file_name !='1_1_search.log')& (file_count.file_name !='1_2_search.log')&\\\n",
    "#                               (file_count.file_name !='1_3_search.log')&(file_count.file_name !='1_4_search.log'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170330_3_search.log</td>\n",
       "      <td>836479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_1_search.log</td>\n",
       "      <td>685870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170331_3_search.log</td>\n",
       "      <td>250377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30_2_sea</td>\n",
       "      <td>242788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170330_2_search.log</td>\n",
       "      <td>242788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1_2_search.log</td>\n",
       "      <td>213449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30_1_sea</td>\n",
       "      <td>147889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20170330_1_search.log</td>\n",
       "      <td>147889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20170331_2_search.log</td>\n",
       "      <td>144227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20170401_3_search.log</td>\n",
       "      <td>139379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1_3_search.log</td>\n",
       "      <td>133699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20170402_3_search.log</td>\n",
       "      <td>122219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1_4_search.log</td>\n",
       "      <td>121626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20170331_1_search.log</td>\n",
       "      <td>116142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20170401_2_search.log</td>\n",
       "      <td>111176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20170402_2_search.log</td>\n",
       "      <td>106083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20170403_3_search.log</td>\n",
       "      <td>103282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20170401_1_search.log</td>\n",
       "      <td>100506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20170402_1_search.log</td>\n",
       "      <td>99786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20170404_3_search.log</td>\n",
       "      <td>98451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20170403_2_search.log</td>\n",
       "      <td>96472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20170404_2_search.log</td>\n",
       "      <td>90750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20170403_1_search.log</td>\n",
       "      <td>89494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20170404_1_search.log</td>\n",
       "      <td>85763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20170408_3_search.log</td>\n",
       "      <td>77101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20170405_3_search.log</td>\n",
       "      <td>76857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20170409_3_search.log</td>\n",
       "      <td>75321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20170408_1_search.log</td>\n",
       "      <td>74856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20170408_2_search.log</td>\n",
       "      <td>74222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20170409_2_search.log</td>\n",
       "      <td>72398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>20170405_1_search.log</td>\n",
       "      <td>72224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>20170405_2_search.log</td>\n",
       "      <td>71240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>20170409_1_search.log</td>\n",
       "      <td>71198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>20170406_3_search.log</td>\n",
       "      <td>70638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>20170407_3_search.log</td>\n",
       "      <td>69877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>20170406_2_search.log</td>\n",
       "      <td>68256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>20170407_1_search.log</td>\n",
       "      <td>66836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>20170407_2_search.log</td>\n",
       "      <td>66216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>20170406_1_search.log</td>\n",
       "      <td>65721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>20170415_3_search.log</td>\n",
       "      <td>65548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 file_name   count\n",
       "0    20170330_3_search.log  836479\n",
       "1           1_1_search.log  685870\n",
       "2    20170331_3_search.log  250377\n",
       "3                 30_2_sea  242788\n",
       "4    20170330_2_search.log  242788\n",
       "5           1_2_search.log  213449\n",
       "6                 30_1_sea  147889\n",
       "7    20170330_1_search.log  147889\n",
       "8    20170331_2_search.log  144227\n",
       "9    20170401_3_search.log  139379\n",
       "10          1_3_search.log  133699\n",
       "11   20170402_3_search.log  122219\n",
       "12          1_4_search.log  121626\n",
       "13   20170331_1_search.log  116142\n",
       "14   20170401_2_search.log  111176\n",
       "15   20170402_2_search.log  106083\n",
       "16   20170403_3_search.log  103282\n",
       "17   20170401_1_search.log  100506\n",
       "18   20170402_1_search.log   99786\n",
       "19   20170404_3_search.log   98451\n",
       "20   20170403_2_search.log   96472\n",
       "21   20170404_2_search.log   90750\n",
       "22   20170403_1_search.log   89494\n",
       "23   20170404_1_search.log   85763\n",
       "24   20170408_3_search.log   77101\n",
       "25   20170405_3_search.log   76857\n",
       "26   20170409_3_search.log   75321\n",
       "27   20170408_1_search.log   74856\n",
       "28   20170408_2_search.log   74222\n",
       "29   20170409_2_search.log   72398\n",
       "30   20170405_1_search.log   72224\n",
       "31   20170405_2_search.log   71240\n",
       "32   20170409_1_search.log   71198\n",
       "33   20170406_3_search.log   70638\n",
       "34   20170407_3_search.log   69877\n",
       "35   20170406_2_search.log   68256\n",
       "36   20170407_1_search.log   66836\n",
       "37   20170407_2_search.log   66216\n",
       "38   20170406_1_search.log   65721\n",
       "39   20170415_3_search.log   65548"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(file_count2.take(40),columns=file_count.columns) ## file count is decreasing over time. why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_valid_uid=valid_uid.join(search_uid_count,on='uid',how='inner').drop(search_uid_count.uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|         uid|count|\n",
      "+------------+-----+\n",
      "|1.67724192E8| 7337|\n",
      "|1.68461664E8| 2988|\n",
      "| 1.6830864E8| 2528|\n",
      "|1.68508992E8| 2429|\n",
      "|1.68742176E8| 2206|\n",
      "|1.68729952E8| 2146|\n",
      "|1.68917264E8| 2019|\n",
      "|1.68742304E8| 1937|\n",
      "| 1.6827864E8| 1891|\n",
      "| 1.6846384E8| 1866|\n",
      "|1.68541936E8| 1833|\n",
      "|1.68743152E8| 1815|\n",
      "| 7.7494248E7| 1753|\n",
      "| 1.6828168E8| 1711|\n",
      "|1.68942976E8| 1654|\n",
      "|1.67750416E8| 1609|\n",
      "| 1.6759232E8| 1592|\n",
      "|1.68472272E8| 1541|\n",
      "| 1.6766336E8| 1522|\n",
      "| 1.6780128E8| 1496|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_valid_uid.show() #looks good! extremely large search count uid have been eliminated after join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning the search_time and convert it to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_df_clean=search_df.select('uid','search_time')\\\n",
    "                          .withColumn('date',search_df.search_time.substr(1,10).cast(DateType()))                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+----------+\n",
      "|         uid|         search_time|      date|\n",
      "+------------+--------------------+----------+\n",
      "| 1.5443664E8|2017-03-01 00:00:24 |2017-03-01|\n",
      "|1.54407264E8|2017-03-01 00:00:53 |2017-03-01|\n",
      "|1.54407856E8|2017-03-01 00:00:54 |2017-03-01|\n",
      "|1.54407248E8|2017-03-01 00:00:55 |2017-03-01|\n",
      "|1.54407328E8|2017-03-01 00:00:55 |2017-03-01|\n",
      "|1.54407248E8|2017-03-01 00:00:56 |2017-03-01|\n",
      "|1.54407264E8|2017-03-01 00:00:59 |2017-03-01|\n",
      "|1.54407264E8|2017-03-01 00:00:59 |2017-03-01|\n",
      "|1.54407552E8|2017-03-01 00:01:00 |2017-03-01|\n",
      "|1.54407248E8|2017-03-01 00:01:02 |2017-03-01|\n",
      "|  1.544072E8|2017-03-01 00:01:06 |2017-03-01|\n",
      "|1.54407248E8|2017-03-01 00:01:06 |2017-03-01|\n",
      "|1.54407264E8|2017-03-01 00:01:07 |2017-03-01|\n",
      "| 1.5440736E8|2017-03-01 00:01:08 |2017-03-01|\n",
      "|1.54407376E8|2017-03-01 00:01:08 |2017-03-01|\n",
      "|1.54407344E8|2017-03-01 00:01:09 |2017-03-01|\n",
      "|1.54407296E8|2017-03-01 00:01:14 |2017-03-01|\n",
      "|1.54407408E8|2017-03-01 00:01:14 |2017-03-01|\n",
      "| 1.5440736E8|2017-03-01 00:01:15 |2017-03-01|\n",
      "|1.54407328E8|2017-03-01 00:01:17 |2017-03-01|\n",
      "+------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_df_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|      date|  count|\n",
      "+----------+-------+\n",
      "|2017-03-01| 685870|\n",
      "|2017-03-02| 213449|\n",
      "|2017-03-03| 133699|\n",
      "|2017-03-04| 121626|\n",
      "|2017-03-30|1617833|\n",
      "|2017-03-31| 510746|\n",
      "|2017-04-01| 351061|\n",
      "|2017-04-02| 328088|\n",
      "|2017-04-03| 289248|\n",
      "|2017-04-04| 274964|\n",
      "|2017-04-05| 220321|\n",
      "|2017-04-06| 204615|\n",
      "|2017-04-07| 202929|\n",
      "|2017-04-08| 226179|\n",
      "|2017-04-09| 218917|\n",
      "|2017-04-10| 176189|\n",
      "|2017-04-11| 114918|\n",
      "|2017-04-12| 163474|\n",
      "|2017-04-13| 167287|\n",
      "|2017-04-14| 162041|\n",
      "+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_df_clean.groupBy('date').count().orderBy('date', ascending=True).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* unusually large count in 03/30. the same problem as the download file. file dating might have some problems,  I will exclude the first 4 days' records of March but keep 03/30 to make it consistent with the download file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_df_clean2=search_df_clean.filter(search_df_clean.date>='2017-03-30').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2017-03-30|\n",
      "|2017-03-31|\n",
      "|2017-04-01|\n",
      "|2017-04-02|\n",
      "|2017-04-03|\n",
      "|2017-04-04|\n",
      "|2017-04-05|\n",
      "|2017-04-06|\n",
      "|2017-04-07|\n",
      "|2017-04-08|\n",
      "|2017-04-09|\n",
      "|2017-04-10|\n",
      "|2017-04-11|\n",
      "|2017-04-12|\n",
      "|2017-04-13|\n",
      "|2017-04-14|\n",
      "|2017-04-15|\n",
      "|2017-04-16|\n",
      "|2017-04-17|\n",
      "|2017-04-18|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_df_clean2.select('date').distinct().orderBy('date', ascending=True).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97829"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_df_clean2.select('uid').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94924"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_df_clean3=search_df_clean.filter(search_df_clean.date>='2017-03-31')\n",
    "search_df_clean3.select('uid').distinct().count()\n",
    "# if excluding 3/30 file, 3000 uid will be eliminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_df_clean2.select('uid','date').repartition(1).write.csv('./data/search_df_clean',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END of preprocessing "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
